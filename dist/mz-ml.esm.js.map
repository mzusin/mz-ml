{
  "version": 3,
  "sources": ["../src/core/simple-linear-regression.ts"],
  "sourcesContent": ["import { ILinearRegressionOptions } from '../interfaces';\n\n/**\n * Simple Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * Usage:\n * const regression = new SimpleLinearRegression({\n *     learningRate: 0.00001,\n *     epochs: 1000,\n *     points,\n *\n *     epochsCallback: (epoch, epochsCount, gradientM, gradientB) => {\n *         if(epoch % 50 === 0 || epoch === epochsCount) {\n *             console.log(`epochs: ${ epoch } / ${ epochsCount }, m = ${ gradientM }, b = ${ gradientB }`);\n *         }\n *     }\n * });\n *\n * const [m, b] = regression.train();\n * const y = regression.predict(80);\n */\nexport class SimpleLinearRegression {\n\n    options: ILinearRegressionOptions;\n    m: number;\n    b: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n        this.m = 0;\n        this.b = 0;\n    }\n\n    private gradientDescent(m: number, b: number) {\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const n = this.options.points.length;\n\n        for(let i=0; i<n; i++) {\n            const [x, actualValue] = this.options.points[i];\n            const predictedValue = m * x + b;\n\n            const diff = (-2/n) * (actualValue - predictedValue);\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            mGradientSum += x * diff;\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += diff;\n        }\n\n        // new_m = current_m - learning_rate * dE/dm\n        const gradientM = m - this.options.learningRate * mGradientSum;\n\n        // new_b = current_b - learning_rate * dE/db\n        const gradientB = b - this.options.learningRate * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    train() {\n        for(let i=0; i<this.options.epochs; i++) {\n\n            const [gradientM, gradientB] = this.gradientDescent(this.m, this.b);\n\n            if(!!this.options.epochsCallback && (typeof this.options.epochsCallback === 'function')) {\n                this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n            }\n\n            this.m = gradientM;\n            this.b = gradientB;\n        }\n\n        return [this.m, this.b];\n    }\n\n    predict(x: number) {\n        return this.m * x + this.b;\n    }\n}"],
  "mappings": ";;;;;;wKAmDO,IAAMA,EAAN,KAA6B,CAMhC,YAAYC,EAAmC,CAJ/CC,EAAA,gBACAA,EAAA,UACAA,EAAA,UAGI,KAAK,QAAUD,EACf,KAAK,EAAI,EACT,KAAK,EAAI,CACb,CAEQ,gBAAgBE,EAAWC,EAAW,CAC1C,IAAIC,EAAe,EACfC,EAAe,EACbC,EAAI,KAAK,QAAQ,OAAO,OAE9B,QAAQC,EAAE,EAAGA,EAAED,EAAGC,IAAK,CACnB,GAAM,CAACC,EAAGC,CAAW,EAAI,KAAK,QAAQ,OAAOF,CAAC,EACxCG,EAAiBR,EAAIM,EAAIL,EAEzBQ,EAAQ,GAAGL,GAAMG,EAAcC,GAGrCN,GAAgBI,EAAIG,EAGpBN,GAAgBM,CACpB,CAGA,IAAMC,EAAYV,EAAI,KAAK,QAAQ,aAAeE,EAG5CS,EAAYV,EAAI,KAAK,QAAQ,aAAeE,EAElD,MAAO,CAACO,EAAWC,CAAS,CAChC,CAEA,OAAQ,CACJ,QAAQN,EAAE,EAAGA,EAAE,KAAK,QAAQ,OAAQA,IAAK,CAErC,GAAM,CAACK,EAAWC,CAAS,EAAI,KAAK,gBAAgB,KAAK,EAAG,KAAK,CAAC,EAE7D,KAAK,QAAQ,gBAAmB,OAAO,KAAK,QAAQ,gBAAmB,YACxE,KAAK,QAAQ,eAAeN,EAAG,KAAK,QAAQ,OAAQK,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,CAEA,MAAO,CAAC,KAAK,EAAG,KAAK,CAAC,CAC1B,CAEA,QAAQL,EAAW,CACf,OAAO,KAAK,EAAIA,EAAI,KAAK,CAC7B,CACJ",
  "names": ["SimpleLinearRegression", "options", "__publicField", "m", "b", "mGradientSum", "bGradientSum", "n", "i", "x", "actualValue", "predictedValue", "diff", "gradientM", "gradientB"]
}
