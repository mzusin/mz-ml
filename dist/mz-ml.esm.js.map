{
  "version": 3,
  "sources": ["../src/core/linear-regression.ts"],
  "sourcesContent": ["import { ILinearRegressionOptions, Optimization } from '../interfaces';\n\n/**\n * Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * Usage:\n * ------\n * const model = new LinearRegression({\n *     learningRate: 0.00001,\n *     epochs: 1000,\n *     points,\n *\n *     epochsCallback: (epoch, epochsCount, gradientM, gradientB) => {\n *         if(epoch % 50 === 0 || epoch === epochsCount) {\n *             console.log(`epochs: ${ epoch } / ${ epochsCount }, m = ${ gradientM }, b = ${ gradientB }`);\n *         }\n *     }\n * });\n *\n * const [m, b] = model.train();\n * const y = model.predict(80);\n */\nexport class LinearRegression {\n\n    options: ILinearRegressionOptions;\n    m: number;\n    b: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n        this.m = 0;\n        this.b = 0;\n    }\n\n    private shuffle() {\n        for (let i = this.options.points.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [this.options.points[i], this.options.points[j]] = [this.options.points[j], this.options.points[i]];\n        }\n    }\n\n    private gradientDescent(batch: [number, number][]) {\n\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const batchSize = batch.length;\n\n        for (const [x, actualValue] of batch) {\n\n            const predictedValue = this.m * x + this.b;\n\n            const diff = actualValue - predictedValue;\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            mGradientSum += -2 * x * diff;\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += -2 * diff;\n        }\n\n        // new_m = current_m - learning_rate * dE/dm\n        const gradientM = this.m - (this.options.learningRate / batchSize) * mGradientSum;\n\n        // new_b = current_b - learning_rate * dE/db\n        const gradientB = this.b - (this.options.learningRate / batchSize) * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    private getBatchSize() {\n        switch (this.options.optimization) {\n            case Optimization.StochasticGradientDescent: {\n                return 1;\n            }\n\n            case Optimization.MiniBatchGradientDescent: {\n                return this.options.batchSize ?? this.options.points.length;\n            }\n\n            default: {\n                return this.options.points.length;\n            }\n        }\n    }\n\n    train() {\n        const batchSize = this.getBatchSize();\n\n        for(let i = 0; i < this.options.epochs; i++) {\n\n            if (this.options.shuffle) {\n                this.shuffle();\n            }\n\n            for (let j = 0; j < this.options.points.length; j += batchSize) {\n                const batch = this.options.points.slice(j, j + batchSize);\n                const [gradientM, gradientB] = this.gradientDescent(batch);\n\n                if (typeof this.options.epochsCallback === 'function') {\n                    this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                }\n\n                this.m = gradientM;\n                this.b = gradientB;\n            }\n        }\n\n        return [this.m, this.b];\n    }\n\n    predict(x: number) {\n        return this.m * x + this.b;\n    }\n}"],
  "mappings": ";;;;;;wKAoDO,IAAMA,EAAN,KAAuB,CAM1B,YAAYC,EAAmC,CAJ/CC,EAAA,gBACAA,EAAA,UACAA,EAAA,UAGI,KAAK,QAAUD,EACf,KAAK,EAAI,EACT,KAAK,EAAI,CACb,CAEQ,SAAU,CACd,QAASE,EAAI,KAAK,QAAQ,OAAO,OAAS,EAAGA,EAAI,EAAGA,IAAK,CACrD,IAAMC,EAAI,KAAK,MAAM,KAAK,OAAO,GAAKD,EAAI,EAAE,EAC5C,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOC,CAAC,CAAC,EAAI,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOD,CAAC,CAAC,CACtG,CACJ,CAEQ,gBAAgBE,EAA2B,CAE/C,IAAIC,EAAe,EACfC,EAAe,EACbC,EAAYH,EAAM,OAExB,OAAW,CAACI,EAAGC,CAAW,IAAKL,EAAO,CAElC,IAAMM,EAAiB,KAAK,EAAIF,EAAI,KAAK,EAEnCG,EAAOF,EAAcC,EAG3BL,GAAgB,GAAKG,EAAIG,EAGzBL,GAAgB,GAAKK,CACzB,CAGA,IAAMC,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeL,EAAaF,EAG/DQ,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeN,EAAaD,EAErE,MAAO,CAACM,EAAWC,CAAS,CAChC,CAEQ,cAAe,CAnG3B,IAAAC,EAoGQ,OAAQ,KAAK,QAAQ,aAAc,CAC/B,OACI,MAAO,GAGX,OACI,OAAOA,EAAA,KAAK,QAAQ,YAAb,KAAAA,EAA0B,KAAK,QAAQ,OAAO,OAGzD,QACI,OAAO,KAAK,QAAQ,OAAO,MAEnC,CACJ,CAEA,OAAQ,CACJ,IAAMP,EAAY,KAAK,aAAa,EAEpC,QAAQL,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAQA,IAAK,CAErC,KAAK,QAAQ,SACb,KAAK,QAAQ,EAGjB,QAASC,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAO,OAAQA,GAAKI,EAAW,CAC5D,IAAMH,EAAQ,KAAK,QAAQ,OAAO,MAAMD,EAAGA,EAAII,CAAS,EAClD,CAACK,EAAWC,CAAS,EAAI,KAAK,gBAAgBT,CAAK,EAErD,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAeF,EAAG,KAAK,QAAQ,OAAQU,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,CACJ,CAEA,MAAO,CAAC,KAAK,EAAG,KAAK,CAAC,CAC1B,CAEA,QAAQL,EAAW,CACf,OAAO,KAAK,EAAIA,EAAI,KAAK,CAC7B,CACJ",
  "names": ["LinearRegression", "options", "__publicField", "i", "j", "batch", "mGradientSum", "bGradientSum", "batchSize", "x", "actualValue", "predictedValue", "diff", "gradientM", "gradientB", "_a"]
}
