{
  "version": 3,
  "sources": ["../src/core/linear-regression.ts"],
  "sourcesContent": ["import { ILinearRegressionOptions, Optimization } from '../interfaces';\n\n/**\n * Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * Usage:\n * ------\n * const model = new LinearRegression({\n *     learningRate: 0.00001,\n *     epochs: 1000,\n *     points,\n *\n *     epochsCallback: (epoch, epochsCount, gradientM, gradientB) => {\n *         if(epoch % 50 === 0 || epoch === epochsCount) {\n *             console.log(`epochs: ${ epoch } / ${ epochsCount }, m = ${ gradientM }, b = ${ gradientB }`);\n *         }\n *     }\n * });\n *\n * const [m, b] = model.train();\n * const y = model.predict(80);\n */\nexport class LinearRegression {\n\n    options: ILinearRegressionOptions;\n    m: number;\n    b: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n        this.m = 0;\n        this.b = 0;\n    }\n\n    private shuffle() {\n        for (let i = this.options.points.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [this.options.points[i], this.options.points[j]] = [this.options.points[j], this.options.points[i]];\n        }\n    }\n\n    private gradientDescent() {\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const n = this.options.points.length;\n\n        for(let i=0; i<n; i++) {\n            const [x, actualValue] = this.options.points[i];\n            const predictedValue = this.m * x + this.b;\n\n            const diff = (-2/n) * (actualValue - predictedValue);\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            mGradientSum += x * diff;\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += diff;\n        }\n\n        // new_m = current_m - learning_rate * dE/dm\n        const gradientM = this.m - this.options.learningRate * mGradientSum;\n\n        // new_b = current_b - learning_rate * dE/db\n        const gradientB = this.b - this.options.learningRate * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    private miniBatchGradientDescent(batch: [number, number][]) {\n\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const batchSize = batch.length;\n\n        for (const [x, actualValue] of batch) {\n            const predictedValue = this.m * x + this.b;\n            const diff = actualValue - predictedValue;\n            mGradientSum += -2 * x * diff;\n            bGradientSum += -2 * diff;\n        }\n\n        const gradientM = this.m - (this.options.learningRate / batchSize) * mGradientSum;\n        const gradientB = this.b - (this.options.learningRate / batchSize) * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    train() {\n\n        const batchSize = this.options.optimization === Optimization.MiniBatchGradientDescent ?\n                            (this.options.batchSize ?? this.options.points.length) : 1;\n\n        for(let i = 0; i < this.options.epochs; i++) {\n\n            if (this.options.shuffle) {\n                this.shuffle();\n            }\n\n            if(this.options.optimization === Optimization.StochasticGradientDescent ||\n                this.options.optimization === Optimization.MiniBatchGradientDescent) {\n\n                for (let j = 0; j < this.options.points.length; j += batchSize) {\n                    const batch = this.options.points.slice(j, j + batchSize);\n                    const [gradientM, gradientB] = this.miniBatchGradientDescent(batch);\n\n                    if (typeof this.options.epochsCallback === 'function') {\n                        this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                    }\n\n                    this.m = gradientM;\n                    this.b = gradientB;\n                }\n            }\n            else{\n                // Simple Gradient Descent -----------------------------------\n                const [gradientM, gradientB] = this.gradientDescent();\n\n                if(!!this.options.epochsCallback && (typeof this.options.epochsCallback === 'function')) {\n                    this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                }\n\n                this.m = gradientM;\n                this.b = gradientB;\n            }\n        }\n\n        return [this.m, this.b];\n    }\n\n    predict(x: number) {\n        return this.m * x + this.b;\n    }\n}"],
  "mappings": ";;;;;;wKAoDO,IAAMA,EAAN,KAAuB,CAM1B,YAAYC,EAAmC,CAJ/CC,EAAA,gBACAA,EAAA,UACAA,EAAA,UAGI,KAAK,QAAUD,EACf,KAAK,EAAI,EACT,KAAK,EAAI,CACb,CAEQ,SAAU,CACd,QAASE,EAAI,KAAK,QAAQ,OAAO,OAAS,EAAGA,EAAI,EAAGA,IAAK,CACrD,IAAMC,EAAI,KAAK,MAAM,KAAK,OAAO,GAAKD,EAAI,EAAE,EAC5C,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOC,CAAC,CAAC,EAAI,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOD,CAAC,CAAC,CACtG,CACJ,CAEQ,iBAAkB,CACtB,IAAIE,EAAe,EACfC,EAAe,EACbC,EAAI,KAAK,QAAQ,OAAO,OAE9B,QAAQJ,EAAE,EAAGA,EAAEI,EAAGJ,IAAK,CACnB,GAAM,CAACK,EAAGC,CAAW,EAAI,KAAK,QAAQ,OAAON,CAAC,EACxCO,EAAiB,KAAK,EAAIF,EAAI,KAAK,EAEnCG,EAAQ,GAAGJ,GAAME,EAAcC,GAGrCL,GAAgBG,EAAIG,EAGpBL,GAAgBK,CACpB,CAGA,IAAMC,EAAY,KAAK,EAAI,KAAK,QAAQ,aAAeP,EAGjDQ,EAAY,KAAK,EAAI,KAAK,QAAQ,aAAeP,EAEvD,MAAO,CAACM,EAAWC,CAAS,CAChC,CAEQ,yBAAyBC,EAA2B,CAExD,IAAIT,EAAe,EACfC,EAAe,EACbS,EAAYD,EAAM,OAExB,OAAW,CAACN,EAAGC,CAAW,IAAKK,EAAO,CAClC,IAAMJ,EAAiB,KAAK,EAAIF,EAAI,KAAK,EACnCG,EAAOF,EAAcC,EAC3BL,GAAgB,GAAKG,EAAIG,EACzBL,GAAgB,GAAKK,CACzB,CAEA,IAAMC,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeG,EAAaV,EAC/DQ,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeE,EAAaT,EAErE,MAAO,CAACM,EAAWC,CAAS,CAChC,CAEA,OAAQ,CArHZ,IAAAG,EAuHQ,IAAMD,EAAY,KAAK,QAAQ,eAAiB,GAC3BC,EAAA,KAAK,QAAQ,YAAb,KAAAA,EAA0B,KAAK,QAAQ,OAAO,OAAU,EAE7E,QAAQb,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAQA,IAMpC,GAJI,KAAK,QAAQ,SACb,KAAK,QAAQ,EAGd,KAAK,QAAQ,eAAiB,GAC7B,KAAK,QAAQ,eAAiB,EAE9B,QAASC,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAO,OAAQA,GAAKW,EAAW,CAC5D,IAAMD,EAAQ,KAAK,QAAQ,OAAO,MAAMV,EAAGA,EAAIW,CAAS,EAClD,CAACH,EAAWC,CAAS,EAAI,KAAK,yBAAyBC,CAAK,EAE9D,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAeX,EAAG,KAAK,QAAQ,OAAQS,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,KAEA,CAEA,GAAM,CAACD,EAAWC,CAAS,EAAI,KAAK,gBAAgB,EAE/C,KAAK,QAAQ,gBAAmB,OAAO,KAAK,QAAQ,gBAAmB,YACxE,KAAK,QAAQ,eAAeV,EAAG,KAAK,QAAQ,OAAQS,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,CAGJ,MAAO,CAAC,KAAK,EAAG,KAAK,CAAC,CAC1B,CAEA,QAAQL,EAAW,CACf,OAAO,KAAK,EAAIA,EAAI,KAAK,CAC7B,CACJ",
  "names": ["LinearRegression", "options", "__publicField", "i", "j", "mGradientSum", "bGradientSum", "n", "x", "actualValue", "predictedValue", "diff", "gradientM", "gradientB", "batch", "batchSize", "_a"]
}
