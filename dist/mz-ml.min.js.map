{
  "version": 3,
  "sources": ["../src/core/linear-regression.ts", "../src/index.ts"],
  "sourcesContent": ["import { ILinearRegressionOptions, Optimization } from '../interfaces';\n\n/**\n * Simple Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * Usage:\n * ------\n * const model = new LinearRegression({\n *     learningRate: 0.00001,\n *     epochs: 1000,\n *     points,\n *\n *     epochsCallback: (epoch, epochsCount, gradientM, gradientB) => {\n *         if(epoch % 50 === 0 || epoch === epochsCount) {\n *             console.log(`epochs: ${ epoch } / ${ epochsCount }, m = ${ gradientM }, b = ${ gradientB }`);\n *         }\n *     }\n * });\n *\n * const [m, b] = model.train();\n * const y = model.predict(80);\n */\nexport class LinearRegression {\n\n    options: ILinearRegressionOptions;\n    m: number;\n    b: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n        this.m = 0;\n        this.b = 0;\n    }\n\n    private shuffle() {\n        for (let i = this.options.points.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [this.options.points[i], this.options.points[j]] = [this.options.points[j], this.options.points[i]];\n        }\n    }\n\n    private gradientDescent(m: number, b: number) {\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const n = this.options.points.length;\n\n        for(let i=0; i<n; i++) {\n            const [x, actualValue] = this.options.points[i];\n            const predictedValue = m * x + b;\n\n            const diff = (-2/n) * (actualValue - predictedValue);\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            mGradientSum += x * diff;\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += diff;\n        }\n\n        // new_m = current_m - learning_rate * dE/dm\n        const gradientM = m - this.options.learningRate * mGradientSum;\n\n        // new_b = current_b - learning_rate * dE/db\n        const gradientB = b - this.options.learningRate * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    private stochasticGradientDescent(x: number, actualValue: number) {\n        const predictedValue = this.m * x + this.b;\n\n        const diff = actualValue - predictedValue;\n        const gradientM = -2 * x * diff;\n        const gradientB = -2 * diff;\n\n        const newM = this.m - this.options.learningRate * gradientM;\n        const newB = this.b - this.options.learningRate * gradientB;\n\n        return [newM, newB];\n    }\n\n    private miniBatchGradientDescent(batch: [number, number][]) {\n\n        let mGradientSum = 0;\n        let bGradientSum = 0;\n        const batchSize = batch.length;\n\n        for (const [x, actualValue] of batch) {\n            const predictedValue = this.m * x + this.b;\n            const diff = actualValue - predictedValue;\n            mGradientSum += -2 * x * diff;\n            bGradientSum += -2 * diff;\n        }\n\n        const gradientM = this.m - (this.options.learningRate / batchSize) * mGradientSum;\n        const gradientB = this.b - (this.options.learningRate / batchSize) * bGradientSum;\n\n        return [gradientM, gradientB];\n    }\n\n    train() {\n\n        const batchSize = this.options.batchSize ?? this.options.points.length;\n\n        for(let i = 0; i < this.options.epochs; i++) {\n            switch (this.options.optimization) {\n                case Optimization.StochasticGradientDescent: {\n\n                    // Stochastic Gradient Descent -------------------------------\n\n                    // Shuffle the data for each epoch if needed\n                    if (this.options.shuffle) {\n                        this.shuffle();\n                    }\n\n                    for (const [x, actualValue] of this.options.points) {\n                        const [gradientM, gradientB] = this.stochasticGradientDescent(x, actualValue);\n\n                        if (typeof this.options.epochsCallback === 'function') {\n                            this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                        }\n\n                        this.m = gradientM;\n                        this.b = gradientB;\n                    }\n\n                    break;\n                }\n\n                case Optimization.MiniBatchGradientDescent: {\n\n                    // Mini Batch Gradient Descent --------------------------------\n\n                    // Shuffle the data for each epoch if needed\n                    if (this.options.shuffle) {\n                        this.shuffle();\n                    }\n\n                    // Split data into mini-batches\n                    for (let j = 0; j < this.options.points.length; j += batchSize) {\n                        const batch = this.options.points.slice(j, j + batchSize);\n                        const [gradientM, gradientB] = this.miniBatchGradientDescent(batch);\n\n                        if (typeof this.options.epochsCallback === 'function') {\n                            this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                        }\n\n                        this.m = gradientM;\n                        this.b = gradientB;\n                    }\n\n                    break;\n                }\n\n                default: {\n                    // Simple Gradient Descent -----------------------------------\n                    const [gradientM, gradientB] = this.gradientDescent(this.m, this.b);\n\n                    if(!!this.options.epochsCallback && (typeof this.options.epochsCallback === 'function')) {\n                        this.options.epochsCallback(i, this.options.epochs, gradientM, gradientB);\n                    }\n\n                    this.m = gradientM;\n                    this.b = gradientB;\n\n                    break;\n                }\n            }\n        }\n\n        return [this.m, this.b];\n    }\n\n    predict(x: number) {\n        return this.m * x + this.b;\n    }\n}", "import * as LinearRegression from './core/linear-regression';\n\nconst api = {\n    ...LinearRegression,\n};\n\ndeclare global {\n    interface Window {\n        mzMl: typeof api,\n    }\n}\n\nwindow.mzMl = window.mzMl || api;\n\nexport * from './core/linear-regression';"],
  "mappings": ";;;;;;idAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,sBAAAE,IAoDO,IAAMC,EAAN,KAAuB,CAM1B,YAAYC,EAAmC,CAJ/CC,EAAA,gBACAA,EAAA,UACAA,EAAA,UAGI,KAAK,QAAUD,EACf,KAAK,EAAI,EACT,KAAK,EAAI,CACb,CAEQ,SAAU,CACd,QAASE,EAAI,KAAK,QAAQ,OAAO,OAAS,EAAGA,EAAI,EAAGA,IAAK,CACrD,IAAMC,EAAI,KAAK,MAAM,KAAK,OAAO,GAAKD,EAAI,EAAE,EAC5C,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOC,CAAC,CAAC,EAAI,CAAC,KAAK,QAAQ,OAAOA,CAAC,EAAG,KAAK,QAAQ,OAAOD,CAAC,CAAC,CACtG,CACJ,CAEQ,gBAAgBE,EAAWC,EAAW,CAC1C,IAAIC,EAAe,EACfC,EAAe,EACbC,EAAI,KAAK,QAAQ,OAAO,OAE9B,QAAQN,EAAE,EAAGA,EAAEM,EAAGN,IAAK,CACnB,GAAM,CAACO,EAAGC,CAAW,EAAI,KAAK,QAAQ,OAAOR,CAAC,EACxCS,EAAiBP,EAAIK,EAAIJ,EAEzBO,EAAQ,GAAGJ,GAAME,EAAcC,GAGrCL,GAAgBG,EAAIG,EAGpBL,GAAgBK,CACpB,CAGA,IAAMC,EAAYT,EAAI,KAAK,QAAQ,aAAeE,EAG5CQ,EAAYT,EAAI,KAAK,QAAQ,aAAeE,EAElD,MAAO,CAACM,EAAWC,CAAS,CAChC,CAEQ,0BAA0BL,EAAWC,EAAqB,CAC9D,IAAMC,EAAiB,KAAK,EAAIF,EAAI,KAAK,EAEnCG,EAAOF,EAAcC,EACrBE,EAAY,GAAKJ,EAAIG,EACrBE,EAAY,GAAKF,EAEjBG,EAAO,KAAK,EAAI,KAAK,QAAQ,aAAeF,EAC5CG,EAAO,KAAK,EAAI,KAAK,QAAQ,aAAeF,EAElD,MAAO,CAACC,EAAMC,CAAI,CACtB,CAEQ,yBAAyBC,EAA2B,CAExD,IAAIX,EAAe,EACfC,EAAe,EACbW,EAAYD,EAAM,OAExB,OAAW,CAACR,EAAGC,CAAW,IAAKO,EAAO,CAClC,IAAMN,EAAiB,KAAK,EAAIF,EAAI,KAAK,EACnCG,EAAOF,EAAcC,EAC3BL,GAAgB,GAAKG,EAAIG,EACzBL,GAAgB,GAAKK,CACzB,CAEA,IAAMC,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeK,EAAaZ,EAC/DQ,EAAY,KAAK,EAAK,KAAK,QAAQ,aAAeI,EAAaX,EAErE,MAAO,CAACM,EAAWC,CAAS,CAChC,CAEA,OAAQ,CAlIZ,IAAAK,EAoIQ,IAAMD,GAAYC,EAAA,KAAK,QAAQ,YAAb,KAAAA,EAA0B,KAAK,QAAQ,OAAO,OAEhE,QAAQjB,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAQA,IACpC,OAAQ,KAAK,QAAQ,aAAc,CAC/B,OAA6C,CAKrC,KAAK,QAAQ,SACb,KAAK,QAAQ,EAGjB,OAAW,CAACO,EAAGC,CAAW,IAAK,KAAK,QAAQ,OAAQ,CAChD,GAAM,CAACG,EAAWC,CAAS,EAAI,KAAK,0BAA0BL,EAAGC,CAAW,EAExE,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAeR,EAAG,KAAK,QAAQ,OAAQW,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,CAEA,KACJ,CAEA,OAA4C,CAKpC,KAAK,QAAQ,SACb,KAAK,QAAQ,EAIjB,QAASX,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAO,OAAQA,GAAKe,EAAW,CAC5D,IAAMD,EAAQ,KAAK,QAAQ,OAAO,MAAMd,EAAGA,EAAIe,CAAS,EAClD,CAACL,EAAWC,CAAS,EAAI,KAAK,yBAAyBG,CAAK,EAE9D,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAef,EAAG,KAAK,QAAQ,OAAQW,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,CACb,CAEA,KACJ,CAEA,QAAS,CAEL,GAAM,CAACD,EAAWC,CAAS,EAAI,KAAK,gBAAgB,KAAK,EAAG,KAAK,CAAC,EAE7D,KAAK,QAAQ,gBAAmB,OAAO,KAAK,QAAQ,gBAAmB,YACxE,KAAK,QAAQ,eAAeZ,EAAG,KAAK,QAAQ,OAAQW,EAAWC,CAAS,EAG5E,KAAK,EAAID,EACT,KAAK,EAAIC,EAET,KACJ,CACJ,CAGJ,MAAO,CAAC,KAAK,EAAG,KAAK,CAAC,CAC1B,CAEA,QAAQL,EAAW,CACf,OAAO,KAAK,EAAIA,EAAI,KAAK,CAC7B,CACJ,EC5MA,IAAMW,EAAMC,EAAA,GACLC,GASP,OAAO,KAAO,OAAO,MAAQF",
  "names": ["linear_regression_exports", "__export", "LinearRegression", "LinearRegression", "options", "__publicField", "i", "j", "m", "b", "mGradientSum", "bGradientSum", "n", "x", "actualValue", "predictedValue", "diff", "gradientM", "gradientB", "newM", "newB", "batch", "batchSize", "_a", "api", "__spreadValues", "linear_regression_exports"]
}
