{
  "version": 3,
  "sources": ["../src/core/linear-regression.ts", "../src/index.ts"],
  "sourcesContent": ["import { ILinearRegression } from '../types';\n\n/**\n * Usage:\n * --------\n * const regression = LinearRegression(0.01, 1000);\n *\n * const x = [1, 2, 3, 4, 5];\n * const y = [2, 4, 5, 4, 5];\n * regression.train(x, y);\n *\n * console.log('Theta0:', regression.getYIntercept());\n * console.log('Theta1:', regression.getSlope());\n * console.log('Prediction for x=6:', regression.predict(6));\n */\nexport const LinearRegression = (learningRate: number, iterations: number) : ILinearRegression => {\n\n    let yIntercept = 0;\n    let slope = 0;\n\n    /**\n     * Gradient descent is an optimization algorithm used to minimize a cost function\n     * (also known as loss function).\n     * The goal of gradient descent is to iteratively adjust the parameters (yIntercept and slope in this case)\n     * to minimize this cost function.\n     */\n    const gradientDescentOptimization = (features: number[], targets: number[]) => {\n        let yInterceptSum = 0;\n        let slopeSum = 0;\n        const n = features.length;\n\n        for(let i=0; i<n; i++) {\n\n            const prediction = predict(features[i]);\n            const error = prediction - targets[i];\n\n            yInterceptSum += error;\n            slopeSum += error * features[i];\n        }\n\n        const yInterceptGradient = (2 / n) * yInterceptSum;\n        const slopeGradient = (2 / n) * slopeSum;\n\n        yIntercept -= learningRate * yInterceptGradient;\n        slope -= learningRate * slopeGradient;\n    };\n\n    const train = (features: number[], targets: number[]) => {\n        // TODO: check that x and y has the same length\n        // TODO: batches, epochs\n\n        for(let i=0; i<iterations; i++) {\n            gradientDescentOptimization(features, targets);\n        }\n    };\n\n    const predict = (feature: number) => {\n        return yIntercept + slope * feature;\n    };\n\n    const getYIntercept = () => {\n        return yIntercept;\n    }\n\n    const getSlope = () => {\n        return slope;\n    };\n\n    return {\n        train,\n        predict,\n        getYIntercept,\n        getSlope,\n    }\n};", "import * as LinearRegression from './core/linear-regression';\n\nconst api = {\n    ...LinearRegression,\n};\n\ndeclare global {\n    interface Window {\n        mzMl: typeof api,\n    }\n}\n\nwindow.mzMl = window.mzMl || api;\n\nexport * from './core/linear-regression';"],
  "mappings": ";;;;;;6ZAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,sBAAAE,IAeO,IAAMA,EAAmB,CAACC,EAAsBC,IAA2C,CAE9F,IAAIC,EAAa,EACbC,EAAQ,EAQNC,EAA8B,CAACC,EAAoBC,IAAsB,CAC3E,IAAIC,EAAgB,EAChBC,EAAW,EACTC,EAAIJ,EAAS,OAEnB,QAAQ,EAAE,EAAG,EAAEI,EAAG,IAAK,CAGnB,IAAMC,EADaC,EAAQN,EAAS,CAAC,CAAC,EACXC,EAAQ,CAAC,EAEpCC,GAAiBG,EACjBF,GAAYE,EAAQL,EAAS,CAAC,CAClC,CAEA,IAAMO,EAAsB,EAAIH,EAAKF,EAC/BM,EAAiB,EAAIJ,EAAKD,EAEhCN,GAAcF,EAAeY,EAC7BT,GAASH,EAAea,CAC5B,EAEMC,EAAQ,CAACT,EAAoBC,IAAsB,CAIrD,QAAQS,EAAE,EAAGA,EAAEd,EAAYc,IACvBX,EAA4BC,EAAUC,CAAO,CAErD,EAEMK,EAAWK,GACNd,EAAaC,EAAQa,EAWhC,MAAO,CACH,MAAAF,EACA,QAAAH,EACA,cAXkB,IACXT,EAWP,SARa,IACNC,CAQX,CACJ,ECxEA,IAAMc,EAAMC,EAAA,GACLC,GASP,OAAO,KAAO,OAAO,MAAQF",
  "names": ["linear_regression_exports", "__export", "LinearRegression", "learningRate", "iterations", "yIntercept", "slope", "gradientDescentOptimization", "features", "targets", "yInterceptSum", "slopeSum", "n", "error", "predict", "yInterceptGradient", "slopeGradient", "train", "i", "feature", "api", "__spreadValues", "linear_regression_exports"]
}
