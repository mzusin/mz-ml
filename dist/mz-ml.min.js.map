{
  "version": 3,
  "sources": ["../src/core/linear-regression.ts", "../src/index.ts"],
  "sourcesContent": ["import { ILinearRegressionOptions } from '../interfaces';\n\n/**\n * Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * General Form:\n * ------------\n * y = w1*x1 + w2*x2 + \u2026 + wn*xn + b\n * [w1, ..., wn] = weights, b = bias\n *\n * Usage:\n * ------\n * const model = new LinearRegression({\n *     learningRate: 0.00001,\n *     epochs: 1000,\n *     points,\n *\n *     epochsCallback: (epoch, epochsCount, gradientM, gradientB) => {\n *         if(epoch % 50 === 0 || epoch === epochsCount) {\n *             console.log(`epochs: ${ epoch } / ${ epochsCount }, m = ${ gradientM }, b = ${ gradientB }`);\n *         }\n *     }\n * });\n *\n * const [m, b] = model.train();\n * const y = model.predict(80);\n */\nexport class LinearRegression {\n\n    options: ILinearRegressionOptions;\n    weights: number[];\n    bias: number;\n\n    features: number[][];\n    labels: number[];\n    n: number;\n\n    batchSize: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n\n        this.features = [...this.options.features];\n        this.labels = [...this.options.labels];\n        this.n = this.features.length > 0 ? this.features[0].length : 0;\n\n        // TODO: validate that features count === labels count\n\n        // Initialize weights to zero\n        this.weights = LinearRegression.initZeroArray(this.n);\n        this.weights.length = this.n;\n        this.weights.fill(0);\n\n        this.bias = 0;\n\n        this.batchSize = this.options.batchSize ?? this.features.length;\n    }\n\n    private static initZeroArray(len: number) {\n        const arr: number[] = [];\n        arr.length = len;\n        arr.fill(0);\n        return arr;\n    }\n\n    // TODO: shuffle\n    /*private shuffle() {\n        for (let i = this.options.points.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [this.options.points[i], this.options.points[j]] = [this.options.points[j], this.options.points[i]];\n        }\n    }*/\n\n    private shuffle() {\n        // const indices = Array.from({ length: features.length }, (_, index) => index);\n\n        const indices: number[] = [];\n        for(let i=0; i<this.n; i++) {\n            indices.push(i);\n        }\n\n        for (let i = this.features.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [indices[i], indices[j]] = [indices[j], indices[i]];\n        }\n\n        for (let i = this.features.length - 1; i > 0; i--) {\n            [this.features[i], this.features[indices[i]]] = [this.features[indices[i]], this.features[i]];\n            [this.labels[i], this.labels[indices[i]]] = [this.labels[indices[i]], this.labels[i]];\n        }\n    }\n\n    private gradientDescent(batchFeatures: number[][], batchLabels: number[]) : [ number[], number ] {\n\n        const mGradientSums = LinearRegression.initZeroArray(this.n);\n        let bGradientSum = 0;\n\n        for (let i = 0; i < batchFeatures.length; i++) {\n\n            const _features: number[] = batchFeatures[i];\n\n            const actualValue = batchLabels[i];\n            const predictedValue = this.predict(_features);\n            const diff = actualValue - predictedValue;\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            for (let j = 0; j < this.n; j++) {\n                mGradientSums[j] += -2 * _features[j] * diff;\n            }\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += -2 * diff;\n        }\n\n        // Update weights and bias using learning rate\n        const newWeights = [];\n\n        for(let i=0; i<this.weights.length; i++) {\n            const _weight = this.weights[i];\n\n            // new_m = current_m - learning_rate * dE/dm\n            const gradientM = _weight - (this.options.learningRate / this.batchSize) * mGradientSums[i];\n            newWeights.push(gradientM);\n        }\n\n        // new_b = current_b - learning_rate * dE/db\n        const newBias = this.bias - (this.options.learningRate / this.batchSize) * bGradientSum;\n\n        return [newWeights, newBias];\n    }\n\n    train() {\n        for(let i = 0; i < this.options.epochs; i++) {\n\n            if (this.options.shuffle) {\n                this.shuffle();\n            }\n\n            // Split data into mini-batches\n            for (let j = 0; j < this.features.length; j += this.batchSize) {\n\n                const batchFeatures = this.features.slice(j, j + this.batchSize);\n                const batchLabels = this.labels.slice(j, j + this.batchSize);\n\n                const [newWeights, newBias] = this.gradientDescent(batchFeatures, batchLabels);\n\n                if (typeof this.options.epochsCallback === 'function') {\n                    this.options.epochsCallback(i, this.options.epochs, newWeights, newBias);\n                }\n\n                this.weights = newWeights;\n                this.bias = newBias;\n            }\n        }\n\n        return [this.weights, this.bias];\n    }\n\n    /**\n     * y = w1*x1 + w2*x2 + \u2026 + wn*xn + b\n     */\n    predict(features: number[]) {\n\n        if (features.length !== this.weights.length) {\n            throw new Error('Number of features does not match the number of weights.');\n        }\n\n        // Calculate the dot product of features and weights and add bias\n        // return this.m * x + this.b;\n        let prediction = this.bias;\n\n        for (let i = 0; i < features.length; i++) {\n            prediction += features[i] * this.weights[i];\n        }\n\n        return prediction;\n    }\n}", "import * as LinearRegression from './core/linear-regression';\n\nconst api = {\n    ...LinearRegression,\n};\n\ndeclare global {\n    interface Window {\n        mzMl: typeof api,\n    }\n}\n\nwindow.mzMl = window.mzMl || api;\n\nexport * from './core/linear-regression';"],
  "mappings": ";;;;;;idAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,sBAAAE,IAyDO,IAAMC,EAAN,KAAuB,CAY1B,YAAYC,EAAmC,CAV/CC,EAAA,gBACAA,EAAA,gBACAA,EAAA,aAEAA,EAAA,iBACAA,EAAA,eACAA,EAAA,UAEAA,EAAA,kBAnEJ,IAAAC,EAsEQ,KAAK,QAAUF,EAEf,KAAK,SAAW,CAAC,GAAG,KAAK,QAAQ,QAAQ,EACzC,KAAK,OAAS,CAAC,GAAG,KAAK,QAAQ,MAAM,EACrC,KAAK,EAAI,KAAK,SAAS,OAAS,EAAI,KAAK,SAAS,CAAC,EAAE,OAAS,EAK9D,KAAK,QAAUD,EAAiB,cAAc,KAAK,CAAC,EACpD,KAAK,QAAQ,OAAS,KAAK,EAC3B,KAAK,QAAQ,KAAK,CAAC,EAEnB,KAAK,KAAO,EAEZ,KAAK,WAAYG,EAAA,KAAK,QAAQ,YAAb,KAAAA,EAA0B,KAAK,SAAS,MAC7D,CAEA,OAAe,cAAcC,EAAa,CACtC,IAAMC,EAAgB,CAAC,EACvB,OAAAA,EAAI,OAASD,EACbC,EAAI,KAAK,CAAC,EACHA,CACX,CAUQ,SAAU,CAGd,IAAMC,EAAoB,CAAC,EAC3B,QAAQC,EAAE,EAAGA,EAAE,KAAK,EAAGA,IACnBD,EAAQ,KAAKC,CAAC,EAGlB,QAASA,EAAI,KAAK,SAAS,OAAS,EAAGA,EAAI,EAAGA,IAAK,CAC/C,IAAMC,EAAI,KAAK,MAAM,KAAK,OAAO,GAAKD,EAAI,EAAE,EAC5C,CAACD,EAAQC,CAAC,EAAGD,EAAQE,CAAC,CAAC,EAAI,CAACF,EAAQE,CAAC,EAAGF,EAAQC,CAAC,CAAC,CACtD,CAEA,QAASA,EAAI,KAAK,SAAS,OAAS,EAAGA,EAAI,EAAGA,IAC1C,CAAC,KAAK,SAASA,CAAC,EAAG,KAAK,SAASD,EAAQC,CAAC,CAAC,CAAC,EAAI,CAAC,KAAK,SAASD,EAAQC,CAAC,CAAC,EAAG,KAAK,SAASA,CAAC,CAAC,EAC5F,CAAC,KAAK,OAAOA,CAAC,EAAG,KAAK,OAAOD,EAAQC,CAAC,CAAC,CAAC,EAAI,CAAC,KAAK,OAAOD,EAAQC,CAAC,CAAC,EAAG,KAAK,OAAOA,CAAC,CAAC,CAE5F,CAEQ,gBAAgBE,EAA2BC,EAA8C,CAE7F,IAAMC,EAAgBX,EAAiB,cAAc,KAAK,CAAC,EACvDY,EAAe,EAEnB,QAASL,EAAI,EAAGA,EAAIE,EAAc,OAAQF,IAAK,CAE3C,IAAMM,EAAsBJ,EAAcF,CAAC,EAErCO,EAAcJ,EAAYH,CAAC,EAC3BQ,EAAiB,KAAK,QAAQF,CAAS,EACvCG,EAAOF,EAAcC,EAG3B,QAASP,EAAI,EAAGA,EAAI,KAAK,EAAGA,IACxBG,EAAcH,CAAC,GAAK,GAAKK,EAAUL,CAAC,EAAIQ,EAI5CJ,GAAgB,GAAKI,CACzB,CAGA,IAAMC,EAAa,CAAC,EAEpB,QAAQV,EAAE,EAAGA,EAAE,KAAK,QAAQ,OAAQA,IAAK,CAIrC,IAAMW,EAHU,KAAK,QAAQX,CAAC,EAGD,KAAK,QAAQ,aAAe,KAAK,UAAaI,EAAcJ,CAAC,EAC1FU,EAAW,KAAKC,CAAS,CAC7B,CAGA,IAAMC,EAAU,KAAK,KAAQ,KAAK,QAAQ,aAAe,KAAK,UAAaP,EAE3E,MAAO,CAACK,EAAYE,CAAO,CAC/B,CAEA,OAAQ,CACJ,QAAQZ,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAQA,IAAK,CAErC,KAAK,QAAQ,SACb,KAAK,QAAQ,EAIjB,QAASC,EAAI,EAAGA,EAAI,KAAK,SAAS,OAAQA,GAAK,KAAK,UAAW,CAE3D,IAAMC,EAAgB,KAAK,SAAS,MAAMD,EAAGA,EAAI,KAAK,SAAS,EACzDE,EAAc,KAAK,OAAO,MAAMF,EAAGA,EAAI,KAAK,SAAS,EAErD,CAACS,EAAYE,CAAO,EAAI,KAAK,gBAAgBV,EAAeC,CAAW,EAEzE,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAeH,EAAG,KAAK,QAAQ,OAAQU,EAAYE,CAAO,EAG3E,KAAK,QAAUF,EACf,KAAK,KAAOE,CAChB,CACJ,CAEA,MAAO,CAAC,KAAK,QAAS,KAAK,IAAI,CACnC,CAKA,QAAQC,EAAoB,CAExB,GAAIA,EAAS,SAAW,KAAK,QAAQ,OACjC,MAAM,IAAI,MAAM,0DAA0D,EAK9E,IAAIC,EAAa,KAAK,KAEtB,QAASd,EAAI,EAAGA,EAAIa,EAAS,OAAQb,IACjCc,GAAcD,EAASb,CAAC,EAAI,KAAK,QAAQA,CAAC,EAG9C,OAAOc,CACX,CACJ,EC7MA,IAAMC,EAAMC,EAAA,GACLC,GASP,OAAO,KAAO,OAAO,MAAQF",
  "names": ["linear_regression_exports", "__export", "LinearRegression", "LinearRegression", "options", "__publicField", "_a", "len", "arr", "indices", "i", "j", "batchFeatures", "batchLabels", "mGradientSums", "bGradientSum", "_features", "actualValue", "predictedValue", "diff", "newWeights", "gradientM", "newBias", "features", "prediction", "api", "__spreadValues", "linear_regression_exports"]
}
