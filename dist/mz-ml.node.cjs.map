{
  "version": 3,
  "sources": ["../src/index-esm.ts", "../src/core/linear-regression.ts"],
  "sourcesContent": ["export * from './core/linear-regression';", "import { ILinearRegression } from '../types';\n\n/**\n * Usage:\n * --------\n * const regression = LinearRegression(0.01, 1000);\n *\n * const x = [1, 2, 3, 4, 5];\n * const y = [2, 4, 5, 4, 5];\n * regression.train(x, y);\n *\n * console.log('Theta0:', regression.getYIntercept());\n * console.log('Theta1:', regression.getSlope());\n * console.log('Prediction for x=6:', regression.predict(6));\n */\nexport const LinearRegression = (learningRate: number, iterations: number) : ILinearRegression => {\n\n    let yIntercept = 0;\n    let slope = 0;\n\n    /**\n     * Gradient descent is an optimization algorithm used to minimize a cost function\n     * (also known as loss function).\n     * The goal of gradient descent is to iteratively adjust the parameters (yIntercept and slope in this case)\n     * to minimize this cost function.\n     */\n    const gradientDescentOptimization = (features: number[], targets: number[]) => {\n        let yInterceptSum = 0;\n        let slopeSum = 0;\n        const n = features.length;\n\n        for(let i=0; i<n; i++) {\n\n            const prediction = predict(features[i]);\n            const error = prediction - targets[i];\n\n            yInterceptSum += error;\n            slopeSum += error * features[i];\n        }\n\n        const yInterceptGradient = (2 / n) * yInterceptSum;\n        const slopeGradient = (2 / n) * slopeSum;\n\n        yIntercept -= learningRate * yInterceptGradient;\n        slope -= learningRate * slopeGradient;\n    };\n\n    const train = (features: number[], targets: number[]) => {\n        // TODO: check that x and y has the same length\n        // TODO: batches, epochs\n\n        for(let i=0; i<iterations; i++) {\n            gradientDescentOptimization(features, targets);\n        }\n    };\n\n    const predict = (feature: number) => {\n        return yIntercept + slope * feature;\n    };\n\n    const getYIntercept = () => {\n        return yIntercept;\n    }\n\n    const getSlope = () => {\n        return slope;\n    };\n\n    return {\n        train,\n        predict,\n        getYIntercept,\n        getSlope,\n    }\n};"],
  "mappings": ";;;;;;4ZAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,sBAAAE,IAAA,eAAAC,EAAAH,GCeO,IAAMI,EAAmB,CAACC,EAAsBC,IAA2C,CAE9F,IAAIC,EAAa,EACbC,EAAQ,EAQNC,EAA8B,CAACC,EAAoBC,IAAsB,CAC3E,IAAIC,EAAgB,EAChBC,EAAW,EACTC,EAAIJ,EAAS,OAEnB,QAAQK,EAAE,EAAGA,EAAED,EAAGC,IAAK,CAGnB,IAAMC,EADaC,EAAQP,EAASK,CAAC,CAAC,EACXJ,EAAQI,CAAC,EAEpCH,GAAiBI,EACjBH,GAAYG,EAAQN,EAASK,CAAC,CAClC,CAEA,IAAMG,EAAsB,EAAIJ,EAAKF,EAC/BO,EAAiB,EAAIL,EAAKD,EAEhCN,GAAcF,EAAea,EAC7BV,GAASH,EAAec,CAC5B,EAEMC,EAAQ,CAACV,EAAoBC,IAAsB,CAIrD,QAAQ,EAAE,EAAG,EAAEL,EAAY,IACvBG,EAA4BC,EAAUC,CAAO,CAErD,EAEMM,EAAWI,GACNd,EAAaC,EAAQa,EAWhC,MAAO,CACH,MAAAD,EACA,QAAAH,EACA,cAXkB,IACXV,EAWP,SARa,IACNC,CAQX,CACJ",
  "names": ["index_esm_exports", "__export", "LinearRegression", "__toCommonJS", "LinearRegression", "learningRate", "iterations", "yIntercept", "slope", "gradientDescentOptimization", "features", "targets", "yInterceptSum", "slopeSum", "n", "i", "error", "predict", "yInterceptGradient", "slopeGradient", "train", "feature"]
}
