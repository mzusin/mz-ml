{
  "version": 3,
  "sources": ["../src/index-esm.ts", "../src/core/linear-regression.ts"],
  "sourcesContent": ["export * from './core/linear-regression';", "import { ILinearRegressionOptions } from '../interfaces';\n\n/**\n * Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n *\n * General Form:\n * ------------\n * y = w1*x1 + w2*x2 + \u2026 + wn*xn + b\n * [w1, ..., wn] = weights, b = bias\n */\nexport class LinearRegression {\n\n    options: ILinearRegressionOptions;\n    weights: number[];\n    bias: number;\n\n    features: number[][];\n    labels: number[];\n    n: number;\n\n    batchSize: number;\n\n    constructor(options: ILinearRegressionOptions) {\n        this.options = options;\n\n        this.features = [...this.options.features];\n        this.labels = [...this.options.labels];\n        this.n = this.features.length > 0 ? this.features[0].length : 0;\n\n        // Initialize weights to zero\n        this.weights = LinearRegression.initZeroArray(this.n);\n        this.weights.length = this.n;\n        this.weights.fill(0);\n\n        this.bias = 0;\n\n        this.batchSize = this.options.batchSize ?? this.features.length;\n    }\n\n    private static initZeroArray(len: number) {\n        const arr: number[] = [];\n        arr.length = len;\n        arr.fill(0);\n        return arr;\n    }\n\n    private shuffle() {\n        const indices: number[] = [];\n        for(let i=0; i<this.n; i++) {\n            indices.push(i);\n        }\n\n        for (let i = this.features.length - 1; i > 0; i--) {\n            const j = Math.floor(Math.random() * (i + 1));\n            [indices[i], indices[j]] = [indices[j], indices[i]];\n        }\n\n        for (let i = this.features.length - 1; i > 0; i--) {\n            [this.features[i], this.features[indices[i]]] = [this.features[indices[i]], this.features[i]];\n            [this.labels[i], this.labels[indices[i]]] = [this.labels[indices[i]], this.labels[i]];\n        }\n    }\n\n    private gradientDescent(batchFeatures: number[][], batchLabels: number[]) : [ number[], number ] {\n\n        const mGradientSums = LinearRegression.initZeroArray(this.n);\n        let bGradientSum = 0;\n\n        for (let i = 0; i < batchFeatures.length; i++) {\n\n            const _features: number[] = batchFeatures[i];\n\n            const actualValue = batchLabels[i];\n            const predictedValue = this.predict(_features);\n            const diff = actualValue - predictedValue;\n\n            // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n            for (let j = 0; j < this.n; j++) {\n                mGradientSums[j] += -2 * _features[j] * diff;\n            }\n\n            // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n            bGradientSum += -2 * diff;\n        }\n\n        // Update weights and bias using learning rate\n        const newWeights = [];\n\n        for(let i=0; i<this.weights.length; i++) {\n            const _weight = this.weights[i];\n\n            // new_m = current_m - learning_rate * dE/dm\n            const gradientM = _weight - (this.options.learningRate / this.batchSize) * mGradientSums[i];\n            newWeights.push(gradientM);\n        }\n\n        // new_b = current_b - learning_rate * dE/db\n        const newBias = this.bias - (this.options.learningRate / this.batchSize) * bGradientSum;\n\n        return [newWeights, newBias];\n    }\n\n    fit() {\n        for(let i = 0; i < this.options.epochs; i++) {\n\n            if (this.options.shuffle) {\n                this.shuffle();\n            }\n\n            // Split data into mini-batches\n            for (let j = 0; j < this.features.length; j += this.batchSize) {\n\n                const batchFeatures = this.features.slice(j, j + this.batchSize);\n                const batchLabels = this.labels.slice(j, j + this.batchSize);\n\n                const [newWeights, newBias] = this.gradientDescent(batchFeatures, batchLabels);\n\n                if (typeof this.options.epochsCallback === 'function') {\n                    this.options.epochsCallback(i, this.options.epochs, newWeights, newBias);\n                }\n\n                this.weights = newWeights;\n                this.bias = newBias;\n            }\n        }\n\n        return [this.weights, this.bias];\n    }\n\n    /**\n     * y = w1*x1 + w2*x2 + \u2026 + wn*xn + b\n     */\n    predict(features: number[]) {\n\n        if (features.length !== this.weights.length) {\n            throw new Error('Number of features does not match the number of weights.');\n        }\n\n        // Calculate the dot product of features and weights and add bias\n        // return this.m * x + this.b;\n        let prediction = this.bias;\n\n        for (let i = 0; i < features.length; i++) {\n            prediction += features[i] * this.weights[i];\n        }\n\n        return prediction;\n    }\n\n    /**\n     * R-squared is the coefficient of determination value,\n     * which measures the goodness of fit of the regression line to the data.\n     * A value close to 1 indicates a perfect fit.\n     * R-Squared range: [0, 1]\n     *\n     * Formula:\n     * --------\n     * R^2 = 1 - (residualSumOfSquares / totalSumOfSquares)\n     * RSS (Residual Sum of Squares) is the sum of squared differences\n     *      between the actual and predicted values\n     * TSS (Total Sum of Squares) is the sum of squared differences between\n     *      the actual values and the mean of the actual values\n     */\n    rSquared() {\n        let residualSumOfSquares = 0; // rss\n        let totalSumOfSquares = 0; // tss\n\n        const meanOfActualValues = this.labels.length <= 0 ? 0 :\n            this.labels.reduce((sum, x) => sum + x) / this.labels.length; // yMean\n\n        for (let i = 0; i < this.features.length; i++) {\n            const actualValue = this.labels[i];\n            const predictedValue = this.predict(this.features[i]);\n\n            residualSumOfSquares += (actualValue - predictedValue) ** 2;\n            totalSumOfSquares += (actualValue - meanOfActualValues) ** 2;\n        }\n\n        return 1 - (residualSumOfSquares / totalSumOfSquares);\n    }\n\n    /**\n     * MSE = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n     * The ideal value of Mean Squared Error (MSE) is 0.\n     * Achieving an MSE of 0 would mean that the model perfectly predicts the target variable\n     * for every data point in the training set. However, it's important to note\n     * that achieving an MSE of exactly 0 is extremely rare and often unrealistic, especially with real-world data.\n     */\n    meanSquaredError() {\n        if(this.features.length <= 0) return 0;\n\n        let mse = 0;\n\n        for (let i = 0; i < this.features.length; i++) {\n            const actualValue = this.labels[i];\n            const predictedValue = this.predict(this.features[i]);\n\n            mse += (actualValue - predictedValue) ** 2;\n        }\n\n        mse /= this.features.length;\n\n        return mse;\n    }\n\n    /**\n     * Compute the Pearson correlation coefficient.\n     * --------------------------------------------\n     * It is a statistical measure that quantifies the strength and direction of the linear relationship\n     * between two variables. It's commonly used to assess the strength of association\n     * between two continuous variables.\n     *\n     * Range [-1, 1]\n     * r=1 indicates a perfect positive linear relationship,\n     *      meaning that as one variable increases, the other variable increases proportionally.\n     *\n     * r=\u22121 indicates a perfect negative linear relationship, meaning that as one variable increases,\n     *      the other variable decreases proportionally.\n     *\n     * r= 0 indicates no linear relationship between the variables.\n     */\n    pearson = () : number[] => {\n        if (this.features.length <= 0 || this.labels.length <= 0) return [];\n\n        const pearsonCoefficients: number[] = [];\n        const yMean = this.labels.reduce((sum, y) => sum + y, 0) / this.labels.length;\n\n        for (let featureIndex = 0; featureIndex < this.n; featureIndex++) {\n            let sumXY = 0; // Sum of the product of (x - xMean) and (y - yMean)\n            let sumX2 = 0; // Sum of squared differences between x and xMean\n            let sumY2 = 0; // Sum of squared differences between y and yMean\n\n            const xValues = this.features.map(feature => feature[featureIndex]);\n            const xMean = xValues.reduce((sum, x) => sum + x, 0) / xValues.length;\n\n            for (let i = 0; i < this.features.length; i++) {\n                const x = this.features[i][featureIndex];\n                const y = this.labels[i];\n\n                sumXY += (x - xMean) * (y - yMean);\n                sumX2 += (x - xMean) ** 2;\n                sumY2 += (y - yMean) ** 2;\n            }\n\n            pearsonCoefficients.push((sumX2 === 0 || sumY2 === 0) ? 0 : (sumXY / Math.sqrt(sumX2 * sumY2)));\n        }\n\n        return pearsonCoefficients;\n    }\n\n}"],
  "mappings": ";;;;;;mjBAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,sBAAAE,IAAA,eAAAC,EAAAH,GCwCO,IAAMI,EAAN,KAAuB,CAY1B,YAAYC,EAAmC,CAV/CC,EAAA,gBACAA,EAAA,gBACAA,EAAA,aAEAA,EAAA,iBACAA,EAAA,eACAA,EAAA,UAEAA,EAAA,kBAyMAA,EAAA,eAAU,IAAiB,CACvB,GAAI,KAAK,SAAS,QAAU,GAAK,KAAK,OAAO,QAAU,EAAG,MAAO,CAAC,EAElE,IAAMC,EAAgC,CAAC,EACjCC,EAAQ,KAAK,OAAO,OAAO,CAACC,EAAKC,IAAMD,EAAMC,EAAG,CAAC,EAAI,KAAK,OAAO,OAEvE,QAASC,EAAe,EAAGA,EAAe,KAAK,EAAGA,IAAgB,CAC9D,IAAIC,EAAQ,EACRC,EAAQ,EACRC,EAAQ,EAENC,EAAU,KAAK,SAAS,IAAIC,GAAWA,EAAQL,CAAY,CAAC,EAC5DM,EAAQF,EAAQ,OAAO,CAACN,EAAKS,IAAMT,EAAMS,EAAG,CAAC,EAAIH,EAAQ,OAE/D,QAASI,EAAI,EAAGA,EAAI,KAAK,SAAS,OAAQA,IAAK,CAC3C,IAAMD,EAAI,KAAK,SAASC,CAAC,EAAER,CAAY,EACjCD,EAAI,KAAK,OAAOS,CAAC,EAEvBP,IAAUM,EAAID,IAAUP,EAAIF,GAC5BK,GAAUO,EAAAF,EAAID,EAAU,GACxBH,GAAUM,EAAAV,EAAIF,EAAU,EAC5B,CAEAD,EAAoB,KAAMM,IAAU,GAAKC,IAAU,EAAK,EAAKF,EAAQ,KAAK,KAAKC,EAAQC,CAAK,CAAE,CAClG,CAEA,OAAOP,CACX,GAtRJ,IAAAc,EAqDQ,KAAK,QAAUhB,EAEf,KAAK,SAAW,CAAC,GAAG,KAAK,QAAQ,QAAQ,EACzC,KAAK,OAAS,CAAC,GAAG,KAAK,QAAQ,MAAM,EACrC,KAAK,EAAI,KAAK,SAAS,OAAS,EAAI,KAAK,SAAS,CAAC,EAAE,OAAS,EAG9D,KAAK,QAAUD,EAAiB,cAAc,KAAK,CAAC,EACpD,KAAK,QAAQ,OAAS,KAAK,EAC3B,KAAK,QAAQ,KAAK,CAAC,EAEnB,KAAK,KAAO,EAEZ,KAAK,WAAYiB,EAAA,KAAK,QAAQ,YAAb,KAAAA,EAA0B,KAAK,SAAS,MAC7D,CAEA,OAAe,cAAcC,EAAa,CACtC,IAAMC,EAAgB,CAAC,EACvB,OAAAA,EAAI,OAASD,EACbC,EAAI,KAAK,CAAC,EACHA,CACX,CAEQ,SAAU,CACd,IAAMC,EAAoB,CAAC,EAC3B,QAAQL,EAAE,EAAGA,EAAE,KAAK,EAAGA,IACnBK,EAAQ,KAAKL,CAAC,EAGlB,QAASA,EAAI,KAAK,SAAS,OAAS,EAAGA,EAAI,EAAGA,IAAK,CAC/C,IAAMM,EAAI,KAAK,MAAM,KAAK,OAAO,GAAKN,EAAI,EAAE,EAC5C,CAACK,EAAQL,CAAC,EAAGK,EAAQC,CAAC,CAAC,EAAI,CAACD,EAAQC,CAAC,EAAGD,EAAQL,CAAC,CAAC,CACtD,CAEA,QAASA,EAAI,KAAK,SAAS,OAAS,EAAGA,EAAI,EAAGA,IAC1C,CAAC,KAAK,SAASA,CAAC,EAAG,KAAK,SAASK,EAAQL,CAAC,CAAC,CAAC,EAAI,CAAC,KAAK,SAASK,EAAQL,CAAC,CAAC,EAAG,KAAK,SAASA,CAAC,CAAC,EAC5F,CAAC,KAAK,OAAOA,CAAC,EAAG,KAAK,OAAOK,EAAQL,CAAC,CAAC,CAAC,EAAI,CAAC,KAAK,OAAOK,EAAQL,CAAC,CAAC,EAAG,KAAK,OAAOA,CAAC,CAAC,CAE5F,CAEQ,gBAAgBO,EAA2BC,EAA8C,CAE7F,IAAMC,EAAgBxB,EAAiB,cAAc,KAAK,CAAC,EACvDyB,EAAe,EAEnB,QAASV,EAAI,EAAGA,EAAIO,EAAc,OAAQP,IAAK,CAE3C,IAAMW,EAAsBJ,EAAcP,CAAC,EAErCY,EAAcJ,EAAYR,CAAC,EAC3Ba,EAAiB,KAAK,QAAQF,CAAS,EACvCG,EAAOF,EAAcC,EAG3B,QAASP,EAAI,EAAGA,EAAI,KAAK,EAAGA,IACxBG,EAAcH,CAAC,GAAK,GAAKK,EAAUL,CAAC,EAAIQ,EAI5CJ,GAAgB,GAAKI,CACzB,CAGA,IAAMC,EAAa,CAAC,EAEpB,QAAQf,EAAE,EAAGA,EAAE,KAAK,QAAQ,OAAQA,IAAK,CAIrC,IAAMgB,EAHU,KAAK,QAAQhB,CAAC,EAGD,KAAK,QAAQ,aAAe,KAAK,UAAaS,EAAcT,CAAC,EAC1Fe,EAAW,KAAKC,CAAS,CAC7B,CAGA,IAAMC,EAAU,KAAK,KAAQ,KAAK,QAAQ,aAAe,KAAK,UAAaP,EAE3E,MAAO,CAACK,EAAYE,CAAO,CAC/B,CAEA,KAAM,CACF,QAAQjB,EAAI,EAAGA,EAAI,KAAK,QAAQ,OAAQA,IAAK,CAErC,KAAK,QAAQ,SACb,KAAK,QAAQ,EAIjB,QAASM,EAAI,EAAGA,EAAI,KAAK,SAAS,OAAQA,GAAK,KAAK,UAAW,CAE3D,IAAMC,EAAgB,KAAK,SAAS,MAAMD,EAAGA,EAAI,KAAK,SAAS,EACzDE,EAAc,KAAK,OAAO,MAAMF,EAAGA,EAAI,KAAK,SAAS,EAErD,CAACS,EAAYE,CAAO,EAAI,KAAK,gBAAgBV,EAAeC,CAAW,EAEzE,OAAO,KAAK,QAAQ,gBAAmB,YACvC,KAAK,QAAQ,eAAeR,EAAG,KAAK,QAAQ,OAAQe,EAAYE,CAAO,EAG3E,KAAK,QAAUF,EACf,KAAK,KAAOE,CAChB,CACJ,CAEA,MAAO,CAAC,KAAK,QAAS,KAAK,IAAI,CACnC,CAKA,QAAQC,EAAoB,CAExB,GAAIA,EAAS,SAAW,KAAK,QAAQ,OACjC,MAAM,IAAI,MAAM,0DAA0D,EAK9E,IAAIC,EAAa,KAAK,KAEtB,QAASnB,EAAI,EAAGA,EAAIkB,EAAS,OAAQlB,IACjCmB,GAAcD,EAASlB,CAAC,EAAI,KAAK,QAAQA,CAAC,EAG9C,OAAOmB,CACX,CAgBA,UAAW,CACP,IAAIC,EAAuB,EACvBC,EAAoB,EAElBC,EAAqB,KAAK,OAAO,QAAU,EAAI,EACjD,KAAK,OAAO,OAAO,CAAChC,EAAKS,IAAMT,EAAMS,CAAC,EAAI,KAAK,OAAO,OAE1D,QAAS,EAAI,EAAG,EAAI,KAAK,SAAS,OAAQ,IAAK,CAC3C,IAAMa,EAAc,KAAK,OAAO,CAAC,EAC3BC,EAAiB,KAAK,QAAQ,KAAK,SAAS,CAAC,CAAC,EAEpDO,GAAyBnB,EAAAW,EAAcC,EAAmB,GAC1DQ,GAAsBpB,EAAAW,EAAcU,EAAuB,EAC/D,CAEA,MAAO,GAAKF,EAAuBC,CACvC,CASA,kBAAmB,CACf,GAAG,KAAK,SAAS,QAAU,EAAG,MAAO,GAErC,IAAIE,EAAM,EAEV,QAASvB,EAAI,EAAGA,EAAI,KAAK,SAAS,OAAQA,IAAK,CAC3C,IAAMY,EAAc,KAAK,OAAOZ,CAAC,EAC3Ba,EAAiB,KAAK,QAAQ,KAAK,SAASb,CAAC,CAAC,EAEpDuB,GAAQtB,EAAAW,EAAcC,EAAmB,EAC7C,CAEA,OAAAU,GAAO,KAAK,SAAS,OAEdA,CACX,CA+CJ",
  "names": ["index_esm_exports", "__export", "LinearRegression", "__toCommonJS", "LinearRegression", "options", "__publicField", "pearsonCoefficients", "yMean", "sum", "y", "featureIndex", "sumXY", "sumX2", "sumY2", "xValues", "feature", "xMean", "x", "i", "__pow", "_a", "len", "arr", "indices", "j", "batchFeatures", "batchLabels", "mGradientSums", "bGradientSum", "_features", "actualValue", "predictedValue", "diff", "newWeights", "gradientM", "newBias", "features", "prediction", "residualSumOfSquares", "totalSumOfSquares", "meanOfActualValues", "mse"]
}
