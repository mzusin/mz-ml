{
  "version": 3,
  "sources": ["../src/index-esm.ts", "../src/core/linear-regression.ts"],
  "sourcesContent": ["export * from './core/linear-regression';", "/**\n * Simple Linear Regression\n *\n * Mean Squared Error (MSE): Error function = Loss function\n * E = (1/n) * sum_from_0_to_n((actual_value - predicted_value)^2)\n * E = (1/n) * sum_from_0_to_n((actual_value - (mx + b))^2)\n * ---------------------------------------------------------\n * Goal: Minimize the error function - find (m, b) with the lowest possible E.\n * How:\n *\n * - Take partial derivative with respect m and also with respect b.\n *   This helps to find the \"m\" that maximally increase E,\n *   and \"b\" that maximally increase E (the steepest ascent).\n *\n * - After we found them, we get the opposite direction\n *   to find the way to decrease E (the steepest descent).\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"m\"?\n * dE/dm = (1/n) * sum_from_0_to_n(2 * (actual_value - (mx + b)) * (-x))\n * dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n * ---------------------------------------------------------\n *\n * How to calculate partial derivative of \"b\"?\n * dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n * ---------------------------------------------------------\n *\n * After the derivatives are found (the steepest ascent)\n * we need to find the steepest descent:\n *\n * new_m = current_m - learning_rate * dE/dm\n * new_b = current_b - learning_rate * dE/db\n */\nexport const gradientDescent = (learningRate: number, m: number, b: number, points: [number, number][]) => {\n\n    let mGradientSum = 0;\n    let bGradientSum = 0;\n    const n = points.length;\n\n    for(let i=0; i<n; i++) {\n        const [x, actualValue] = points[i];\n        const predictedValue = m * x + b;\n\n        const diff = (-2/n) * (actualValue - predictedValue);\n\n        // dE/dm = (-2/n) * sum_from_0_to_n(x * (actual_value - (mx + b)))\n        mGradientSum += x * diff;\n\n        // dE/db = (-2/n) * sum_from_0_to_n(actual_value - (mx + b))\n        bGradientSum += diff;\n    }\n\n    // new_m = current_m - learning_rate * dE/dm\n    const gradientM = m - learningRate * mGradientSum;\n\n    // new_b = current_b - learning_rate * dE/db\n    const gradientB  = b - learningRate * bGradientSum;\n\n    return [gradientM, gradientB];\n};\n\n/**\n * Simple Linear Regression\n * -----------------------------\n * Learning rate can be 0.0001\n * Epochs can be 1000\n */\nexport const SimpleLinearRegression = (learningRate: number, epochs: number, points: [number, number][]) => {\n    let m = 0;\n    let b = 0;\n\n    for(let i=0; i<epochs; i++) {\n        const [gradientM, gradientB] = gradientDescent(learningRate, m, b, points);\n        m = gradientM;\n        b = gradientB;\n    }\n\n    return [m, b];\n};\n\n/*const getPoints = () => {\n\n    // study time ---> exam score\n    return [\n        [10, 70],\n        [12, 75],\n        [8, 54],\n        [40, 99],\n        [1, 11],\n    ];\n};*/\n"],
  "mappings": ";;;;;;4ZAAA,IAAAA,EAAA,GAAAC,EAAAD,EAAA,4BAAAE,EAAA,oBAAAC,IAAA,eAAAC,EAAAJ,GCiCO,IAAMK,EAAkB,CAACC,EAAsBC,EAAWC,EAAWC,IAA+B,CAEvG,IAAIC,EAAe,EACfC,EAAe,EACbC,EAAIH,EAAO,OAEjB,QAAQ,EAAE,EAAG,EAAEG,EAAG,IAAK,CACnB,GAAM,CAACC,EAAGC,CAAW,EAAIL,EAAO,CAAC,EAC3BM,EAAiBR,EAAIM,EAAIL,EAEzBQ,EAAQ,GAAGJ,GAAME,EAAcC,GAGrCL,GAAgBG,EAAIG,EAGpBL,GAAgBK,CACpB,CAGA,IAAMC,EAAYV,EAAID,EAAeI,EAG/BQ,EAAaV,EAAIF,EAAeK,EAEtC,MAAO,CAACM,EAAWC,CAAS,CAChC,EAQaC,EAAyB,CAACb,EAAsBc,EAAgBX,IAA+B,CACxG,IAAIF,EAAI,EACJC,EAAI,EAER,QAAQa,EAAE,EAAGA,EAAED,EAAQC,IAAK,CACxB,GAAM,CAACJ,EAAWC,CAAS,EAAIb,EAAgBC,EAAcC,EAAGC,EAAGC,CAAM,EACzEF,EAAIU,EACJT,EAAIU,CACR,CAEA,MAAO,CAACX,EAAGC,CAAC,CAChB",
  "names": ["index_esm_exports", "__export", "SimpleLinearRegression", "gradientDescent", "__toCommonJS", "gradientDescent", "learningRate", "m", "b", "points", "mGradientSum", "bGradientSum", "n", "x", "actualValue", "predictedValue", "diff", "gradientM", "gradientB", "SimpleLinearRegression", "epochs", "i"]
}
